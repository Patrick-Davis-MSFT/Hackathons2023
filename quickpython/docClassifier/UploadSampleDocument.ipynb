{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classifier\n",
    "\n",
    "> Based on the following documentaion \n",
    "> https://learn.microsoft.com/en-us/python/api/overview/azure/ai-documentintelligence-readme?view=azure-python\n",
    "> https://github.com/szetinglau/CustomClassifier\n",
    "> https://techcommunity.microsoft.com/blog/azure-ai-services-blog/building-a-document-intelligence-custom-classification-model-with-the-python-sdk/4104233\n",
    "\n",
    "## Setup\n",
    "1. Update the inputs as needed\n",
    "1. Create a new Resource Group (tried in US Gov Virginia and US East)\n",
    "1. In the resource group\n",
    "    1. Create a new Multi-Service AI Account\n",
    "    1. A new Storage Account (HOT LRS No Purge Protection) - For demo only\n",
    "1. Assign the following roles\n",
    "    * Cognitive Services Data Contributor - User\n",
    "    * Cognitive Services User roles - User\n",
    "    * Blob Data Owner for both the - User and the Document intellegence\n",
    "    * Owner on the storage account - User\n",
    "1. Login to azure using `az login`\n",
    "1. Prepare the local documents training data as specified in [this link](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/building-a-document-intelligence-custom-classification-model-with-the-python-sdk/4104233)\n",
    "\n",
    "> For Government use `az cloud set --name AzureUSGovernment`. To return to MAC `az cloud set --name AzureCloud` \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classifier ID RUN ONCE ONLY \n",
    "\n",
    "# this will create a new classifier in the service save it once you create it\n",
    "\n",
    "import uuid\n",
    "CLASSIFIER_ID = str(uuid.uuid4())\n",
    "BASE_CLASSIFIER_ID = None\n",
    "print(f\"CLASSIFIER_ID: {CLASSIFIER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, AzureAuthorityHosts, InteractiveBrowserCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "container_name = \"trainingdata\"\n",
    "storage_account_name = \"\"\n",
    "doc_intel_service_name = \"\"\n",
    "local_directory = \"\" # relative directory with no ending slash\n",
    "CLASSIFIER_DESCRIPTION = \"\"\n",
    "TESTING_DOCUMENTS=  \"\"\n",
    "\n",
    "doc_intel_key = None # STATIC -- Do not change\n",
    "authority = AzureAuthorityHosts.AZURE_PUBLIC_CLOUD # STATIC -- Do not change\n",
    "storage_postfix = \"core.windows.net\" # STATIC -- Do not change\n",
    "doc_intel_service_postfix = \"cognitiveservices.azure.com\" # STATIC -- Do not change\n",
    "API_TYPE = \"documentClassifiers\" # STATIC -- Do not change\n",
    "API_VERSION = \"2024-11-30\" # STATIC -- Do not change\n",
    "#uncomment for Gov Cloud\n",
    "#doc intellegence key only needed for gov cloud\n",
    "#doc_intel_key = \"\" \n",
    "\n",
    "#storage_postfix = \"core.usgovcloudapi.net\" # STATIC\n",
    "#authority = AzureAuthorityHosts.AZURE_GOVERNMENT # STATIC\n",
    "#doc_intel_service_postfix = \"cognitiveservices.azure.us\" # STATIC\n",
    "#doc_intel_cred = AzureKeyCredential(doc_intel_key) # STATIC -- Do not change\n",
    "#end uncomment for gov cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import logging, json, os, time\n",
    "from requests import post, get\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, ContainerSasPermissions, generate_container_sas\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient, DocumentIntelligenceAdministrationClient\n",
    "from azure.ai.documentintelligence.models import (\n",
    "                AzureBlobFileListContentSource,\n",
    "                ClassifierDocumentTypeDetails,\n",
    "                BuildDocumentClassifierRequest,\n",
    "            )\n",
    "\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "\n",
    "doc_intel_endpoint = f\"https://{doc_intel_service_name}.{doc_intel_service_postfix}/\"\n",
    "print(f\"doc_intel_endpoint: {doc_intel_endpoint}\")\n",
    "\n",
    "    \n",
    "# Create the credential object\n",
    "# Requires the Cognitive Services Data Reader/Contributor and Cognitive Services User roles\n",
    "credential = DefaultAzureCredential(authority=authority)\n",
    "if (doc_intel_key is None):\n",
    "    doc_intel_cred = credential\n",
    "\n",
    "# Output the current user's login name\n",
    "#interactive_credential = DefaultAzureCredential(authority=authority)\n",
    "user_info = credential.get_token(\"https://management.azure.com/.default\")\n",
    "print(f\"Logged in as: {user_info.token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Files\n",
    "\n",
    "This will take a while for a lot of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layout():\n",
    "# [START analyze_layout]\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=doc_intel_endpoint, credential=doc_intel_cred, audience=f\"https://{doc_intel_service_postfix}\"\n",
    "    )\n",
    "    # Create arrays to store the incompatible files\n",
    "    incompatible_files = []\n",
    "    # Iterate through files in the local directory and analyze each document\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for dir in dirs:\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            for file in os.listdir(dir_path):\n",
    "                document_file_path = os.path.join(dir_path, file)\n",
    "                if not file.endswith((\".json\", \".jsonl\")):\n",
    "                    print(f\"Analyzing document in {document_file_path}\")\n",
    "                    ocr_json_file_path = document_file_path + \".ocr.json\"\n",
    "                    try:\n",
    "                        with open(document_file_path, \"rb\") as f:\n",
    "                            # Use begin_analyze_document to start the analysis process, and use a callback in order to recieve the raw response\n",
    "                            poller = document_intelligence_client.begin_analyze_document(\n",
    "                                \"prebuilt-layout\", body=f, content_type=\"application/octet-stream\", cls=lambda raw_response, _, headers: create_ocr_json(ocr_json_file_path, raw_response)\n",
    "                            )\n",
    "                    except HttpResponseError as error:\n",
    "                        print(f\"Analysis of {file} failed: {error.error}\\n\\nSkipping to next file...\")\n",
    "                        incompatible_files.append(document_file_path)\n",
    "                        break \n",
    "                    result = poller.result()\n",
    "# [END analyze_layout]\n",
    "\n",
    "    # Print the list of incompatible files\n",
    "    if len(incompatible_files) > 0:\n",
    "        print(\"\\nThe following files were skipped as they are corrupted or the format is unsupported:\")\n",
    "        for file in incompatible_files:\n",
    "            print(f\"\\t{file}\")\n",
    "        print(\"Please visit the following link for more information on supported file types and sizes. \\nhttps://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-custom-classifier?view=doc-intel-4.0.0#input-requirements\")\n",
    "    \n",
    "    print(\"Batch layout analysis completed!\")\n",
    "\n",
    "def create_ocr_json(ocr_json_file_path, raw_response):\n",
    "# [START create_ocr_json]\n",
    "    with open(ocr_json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_response.http_response.body().decode(\"utf-8\"))\n",
    "        print(f\"\\tOutput saved to {ocr_json_file_path}\")\n",
    "# [END create_ocr_json]\n",
    "\n",
    "print(\"Batch layout analysis started...\")\n",
    "analyze_layout()\n",
    "print(\"Batch layout analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Container if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Starting to create container\")\n",
    "# Define the connection string and container name for Azure Blob Storage\n",
    "\n",
    "\n",
    "\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.{storage_postfix}\", credential=credential)\n",
    "\n",
    "\n",
    "\n",
    "# Create the container if it doesn't exist\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "    print(f\"Created container {container_name}\")\n",
    "\n",
    "print(\"Done Creating container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Data to Storage Account\n",
    "Python cell to upload the data into a storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Upload Blob\")\n",
    "\n",
    "def upload_documents():\n",
    "# [START upload_documents]\n",
    "    # Create arrays to store the incompatible files\n",
    "    incompatible_files = []\n",
    "\n",
    "    # List all files in the local directory\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for dir in dirs:\n",
    "            jsonl_data = []\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            for file in os.listdir(dir_path):\n",
    "                local_file_path = os.path.join(dir_path, file)\n",
    "                ocr_json_file_path = local_file_path + \".ocr.json\"\n",
    "                if ( file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".heif\", \".pdf\", \".docx\", \".xlsx\", \".pptx\")) \n",
    "                    and os.path.isfile(ocr_json_file_path)):\n",
    "                        upload_file_to_blob(local_file_path, jsonl_data)\n",
    "                        upload_file_to_blob(ocr_json_file_path)\n",
    "                elif not file.endswith((\".ocr.json\", \".jsonl\")):\n",
    "                    incompatible_files.append(local_file_path)\n",
    "\n",
    "            # Write the .jsonl file as long as there are at least 5 training files per document type\n",
    "            if len(jsonl_data) >= 5:\n",
    "                jsonl_file_path = os.path.join(local_directory, f\"{dir}.jsonl\")\n",
    "                print(f\"Getting {jsonl_file_path}\")\n",
    "                with open(jsonl_file_path, \"w\") as f:\n",
    "                    for item in jsonl_data:\n",
    "                        f.write(json.dumps(item) + \"\\n\")\n",
    "                \n",
    "                upload_file_to_blob(jsonl_file_path)\n",
    "\n",
    "    # Print the list of incompatible files\n",
    "    if len(incompatible_files) > 0:\n",
    "        print(\"\\nThe following files are not of a supported file type, missing a corresponding .ocr.json file, or both:\")\n",
    "        for local_file_path in incompatible_files:\n",
    "            print(f\"\\t{local_file_path}\")\n",
    "        print(\"Please ensure you run analyze_layout.py to create .ocr.json files before uploading documents. \\nVisit the following link for more information on supported file types and sizes. \\nhttps://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-custom-classifier?view=doc-intel-4.0.0#input-requirements\")\n",
    "    \n",
    "    print(\"Batch upload completed!\")\n",
    "# [END upload_documents]\n",
    "    \n",
    "def upload_file_to_blob(local_file_path, jsonl_data=None):\n",
    "# [START upload_file_to_blob]\n",
    "    blob_name = os.path.relpath(local_file_path, local_directory).replace(\"\\\\\", \"/\")\n",
    "    if jsonl_data is not None:\n",
    "        jsonl_data.append({\"file\": f\"{blob_name}\"})\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    # Upload the file to Azure Blob Storage\n",
    "    with open(local_file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    print(f\"Uploaded {local_file_path} to {blob_name} in container {container_name}\")\n",
    "# [END upload_file_to_blob]\n",
    "\n",
    "upload_documents()\n",
    "print(\"End Upload Blob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Classifier\n",
    "\n",
    "Build the document classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "# [START build_classifier]\n",
    "    base_classifier_id = BASE_CLASSIFIER_ID\n",
    "    classifier_description = CLASSIFIER_DESCRIPTION\n",
    "    document_model_admin_client = create_clients()\n",
    "    container_sas_url = create_container_sas_url(container_client)\n",
    "\n",
    "    poller = document_model_admin_client.begin_build_classifier(\n",
    "        BuildDocumentClassifierRequest(\n",
    "            classifier_id=CLASSIFIER_ID,\n",
    "            base_classifier_id=base_classifier_id,\n",
    "            description=classifier_description,\n",
    "            doc_types= get_doctypes(container_client, container_sas_url),\n",
    "        )\n",
    "    )\n",
    "    result = poller.result()\n",
    "    print_classifier_results(result)\n",
    "# [END build_classifier]\n",
    "\n",
    "def create_clients():\n",
    "# [START create_clients]\n",
    "    endpoint = doc_intel_endpoint\n",
    "    document_model_admin_client = DocumentIntelligenceAdministrationClient(endpoint=endpoint, \n",
    "                                                                           credential=doc_intel_cred, \n",
    "                                                                           audience=f\"https://{doc_intel_service_postfix}\")\n",
    "    return document_model_admin_client\n",
    "# [END create_clients]\n",
    "\n",
    "\n",
    "def create_container_sas_url(container_client):\n",
    "# [START create_container_sas_url]\n",
    "    # Define the SAS token permissions\n",
    "    sas_permissions=ContainerSasPermissions(read=True, list=True)\n",
    "\n",
    "    # Define the expiry time and start time for the SAS token\n",
    "    start_time = datetime.now(timezone.utc) - timedelta(minutes=1)\n",
    "    expiry_time = datetime.now(timezone.utc) + timedelta(minutes=15)\n",
    "\n",
    "    # Generate the container SAS token\n",
    "    # container_sas_token = generate_container_sas(\n",
    "    #    container_client.account_name,\n",
    "    #    container_client.container_name,\n",
    "    #    account_key=container_client.credential.account_key,\n",
    "    #    permission=sas_permissions,\n",
    "    #    expiry=expiry_time,\n",
    "    #    start=start_time,\n",
    "    #)\n",
    "    # Create the container sas URL by appending the token to the container url\n",
    "    #container_sas_url = f\"{container_client.url}?{container_sas_token}\"\n",
    "    container_sas_url = f\"{container_client.url}\"\n",
    "\n",
    "    return container_sas_url\n",
    "# [END create_container_sas_url]\n",
    "\n",
    "def get_doctypes(container_client, container_sas_url):\n",
    "# [START get_doctypes]\n",
    "    doc_types = {}\n",
    "    doc_types_list = []\n",
    "\n",
    "    blob_list = container_client.walk_blobs()\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith(\".jsonl\"):\n",
    "            doc_type = os.path.splitext(blob.name)[0]\n",
    "            doc_types_list.append(doc_type)\n",
    "\n",
    "    for doc_type in doc_types_list:\n",
    "        doc_types[doc_type] = ClassifierDocumentTypeDetails(\n",
    "            azure_blob_file_list_source=AzureBlobFileListContentSource(\n",
    "                container_url=container_sas_url, \n",
    "                file_list=f\"{doc_type}.jsonl\"\n",
    "            )\n",
    "        )\n",
    "    return doc_types\n",
    "# [END get_doctypes]\n",
    "\n",
    "def print_classifier_results(result):\n",
    "# [START print_classifier_results]\n",
    "    print(f\"Classifier ID: {result.classifier_id}\")\n",
    "    print(f\"API version used to build the classifier model: {result.api_version}\")\n",
    "    print(f\"Classifier description: {result.description}\")\n",
    "    print(f\"Document classes used for training the model:\")\n",
    "    for doc_type in result.doc_types.items():\n",
    "        print(f\"Document type: {doc_type}\")\n",
    "        \\\n",
    "# [START print_classifier_results]\n",
    "\n",
    "build_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify a local document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document(classifier_id, doc_path):\n",
    "    # [START classify_document]\n",
    "\n",
    "    endpoint = doc_intel_endpoint\n",
    "    classifier_id = classifier_id\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint, \n",
    "                                                              credential=doc_intel_cred, \n",
    "                                                              audience=f\"https://{doc_intel_service_postfix}\")\n",
    "    with open(doc_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_classify_document(\n",
    "            classifier_id, body=f, content_type=\"application/pdf\"\n",
    "        )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    print(\"----Classified documents----\")\n",
    "    if result.documents:\n",
    "        for doc in result.documents:\n",
    "            if doc.bounding_regions:\n",
    "                print(\n",
    "                    f\"Found document of type '{doc.doc_type or 'N/A'}' with a confidence of {doc.confidence} contained on \"\n",
    "                    f\"the following pages: {[region.page_number for region in doc.bounding_regions]}\"\n",
    "                )\n",
    "    # [END classify_document]\n",
    "\n",
    "try:\n",
    "\n",
    "    for document in os.listdir(TESTING_DOCUMENTS):\n",
    "        doc_path = os.path.join(TESTING_DOCUMENTS, document)\n",
    "        print(f\"Classifying document {document}...\")\n",
    "        request = classify_document(CLASSIFIER_ID, doc_path)\n",
    "\n",
    "        \n",
    "except HttpResponseError as error:\n",
    "    # Examples of how to check an HttpResponseError\n",
    "    # Check by error code:\n",
    "    if error.error is not None:\n",
    "        if error.error.code == \"InvalidImage\":\n",
    "            print(f\"Received an invalid image error: {error.error}\")\n",
    "        if error.error.code == \"InvalidRequest\":\n",
    "            print(f\"Received an invalid request error: {error.error}\")\n",
    "        # Raise the error again after printing it\n",
    "        raise\n",
    "    # If the inner error is None and then it is possible to check the message to get more information:\n",
    "    if \"Invalid request\".casefold() in error.message.casefold():\n",
    "        print(f\"Uh-oh! Seems there was an invalid request: {error}\")\n",
    "    # Raise the error again\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
